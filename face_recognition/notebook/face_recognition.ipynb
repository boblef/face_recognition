{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import face_recognition\n",
    "from collections import defaultdict\n",
    "from imutils.video import VideoStream\n",
    "import numpy as np\n",
    "from scipy.ndimage import imread\n",
    "from scipy.misc import imresize, imsave\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import AveragePooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import (\n",
    "    TensorBoard,\n",
    "    ModelCheckpoint,\n",
    "    EarlyStopping)\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 24\n",
    "eyes_path = Path('/home/bob/apps/face_recognition/data/images')\n",
    "tsb_path = Path('/home/bob/apps/face_recognition/tsb')\n",
    "checkpoints_path = Path('/home/bob/apps/face_recognition/checkpoints')\n",
    "face_imgs_basepath = Path('/home/bob/apps/face_recognition/data/face_imgs')\n",
    "xml_models_basepath = Path('/home/bob/apps/face_recognition/xml_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd0920cee48>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = cv2.imread(str(eyes_path/Path('closed/closed_eye_0001.jpg_face_1_L.jpg')),\n",
    "                 cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_encode(images):\n",
    "    \"\"\"\n",
    "    Creates encodings of images of the persons I want to recognize\n",
    "    \"\"\"\n",
    "    known_encodings = []\n",
    "    known_names = []\n",
    "    print(\"[LOG] Encoding dataset ...\")\n",
    "\n",
    "    for image_path in tqdm(images):\n",
    "        # Load image\n",
    "        image = cv2.imread(image_path)\n",
    "        # Convert it from BGR to RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "     \n",
    "        # detect face in the image and get its location (square boxes coordinates)\n",
    "        boxes = face_recognition.face_locations(image, model='hog')\n",
    "\n",
    "        # Encode the face into a 128-d embeddings vector\n",
    "        encoding = face_recognition.face_encodings(image, boxes)\n",
    "\n",
    "        # the person's name is the name of the folder where the image comes from\n",
    "        name = image_path.split('/')\n",
    "        name = name[-1].split('_')\n",
    "        name = name[0]\n",
    "\n",
    "        if len(encoding) > 0 : \n",
    "            known_encodings.append(encoding[0])\n",
    "            known_names.append(name)\n",
    "\n",
    "    return {\"encodings\": known_encodings, \"names\": known_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Encoding dataset ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f920c48633c9468894e34ea55001b00d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get imgs path\n",
    "imgs_list = list(face_imgs_basepath.glob('*'))\n",
    "imgs_list = [str(path) for path in imgs_list]\n",
    "imgs_list = sorted(imgs_list)\n",
    "encoding_dict = process_and_encode(imgs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generator\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4362 images belonging to 2 classes.\n",
      "Found 484 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "                    directory=str(eyes_path),\n",
    "                    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "                    color_mode='grayscale',\n",
    "                    classes=['opened', 'closed'],\n",
    "                    class_mode='binary',\n",
    "                    subset='training',\n",
    "                    batch_size=32)\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "                    directory=str(eyes_path),\n",
    "                    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "                    color_mode='grayscale',\n",
    "                    classes=['opened', 'closed'],\n",
    "                    class_mode='binary',\n",
    "                    subset='validation',\n",
    "                    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_generator, val_generator, optimizer):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=6,\n",
    "                     kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=(IMG_SIZE,IMG_SIZE,1)))\n",
    "    model.add(AveragePooling2D())\n",
    "\n",
    "    model.add(Conv2D(filters=16,\n",
    "                     kernel_size=(3, 3),\n",
    "                     activation='relu'))\n",
    "    model.add(AveragePooling2D())\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(units=120,\n",
    "                    activation='relu'))\n",
    "\n",
    "    model.add(Dense(units=84,\n",
    "                    activation='relu'))\n",
    "\n",
    "    model.add(Dense(units=1,\n",
    "                    activation = 'sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "  \n",
    "    print(model.summary())\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsb = TensorBoard(log_dir=str(tsb_path/Path('20190609_tsb01')))\n",
    "\n",
    "mc = ModelCheckpoint(filepath=str(checkpoints_path/Path('20190609_best01')),\n",
    "                     monitor='val_loss',\n",
    "                     verbose=1,\n",
    "                     save_best_only=True,\n",
    "                     save_weights_only=True,\n",
    "                     period=10)\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss',\n",
    "                   patience=50,\n",
    "                   verbose=1)\n",
    "callbacks = [tsb, mc, es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 22, 22, 6)         60        \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 11, 11, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 9, 9, 16)          880       \n",
      "_________________________________________________________________\n",
      "average_pooling2d_2 (Average (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               30840     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 85        \n",
      "=================================================================\n",
      "Total params: 42,029\n",
      "Trainable params: 42,029\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "adam = Adam(lr=0.0003)\n",
    "model = train(train_generator, val_generator, adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID=val_generator.n//val_generator.batch_size\n",
    "model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data=val_generator,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "                    epochs=200,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(str(checkpoints_path/Path('20190609_best01')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isBlinking(history, maxFrames):\n",
    "    \"\"\" @history: A string containing the history of eyes status \n",
    "         where a '1' means that the eyes were closed and '0' open.\n",
    "        @maxFrames: The maximal number of successive frames where an eye is closed \"\"\"\n",
    "    for i in range(maxFrames):\n",
    "        pattern = '1' + '0'*(i+1) + '1'\n",
    "        if pattern in history:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_display(model, video_capture, face_detector, open_eyes_detector, left_eye_detector, right_eye_detector, data, eyes_detected):\n",
    "        frame = video_capture.read()\n",
    "        # resize the frame\n",
    "        frame = cv2.resize(frame, (0, 0), fx=0.6, fy=0.6)\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Detect faces\n",
    "        faces = face_detector.detectMultiScale(\n",
    "            gray,\n",
    "            scaleFactor=1.2,\n",
    "            minNeighbors=5,\n",
    "            minSize=(50, 50),\n",
    "            flags=cv2.CASCADE_SCALE_IMAGE\n",
    "        )\n",
    "\n",
    "        # for each detected face\n",
    "        for (x,y,w,h) in faces:\n",
    "            # Encode the face into a 128-d embeddings vector\n",
    "            encoding = face_recognition.face_encodings(rgb, [(y, x+w, y+h, x)])[0]\n",
    "\n",
    "            # Compare the vector with all known faces encodings\n",
    "            matches = face_recognition.compare_faces(data[\"encodings\"], encoding)\n",
    "\n",
    "            # For now we don't know the person name\n",
    "            name = \"Unknown\"\n",
    "\n",
    "            # If there is at least one match:\n",
    "            if True in matches:\n",
    "                matchedIdxs = [i for (i, b) in enumerate(matches) if b]\n",
    "                counts = {}\n",
    "                for i in matchedIdxs:\n",
    "                    name = data[\"names\"][i]\n",
    "                    counts[name] = counts.get(name, 0) + 1\n",
    "\n",
    "                # The known encoding with the most number of matches corresponds to the detected face name\n",
    "                name = max(counts, key=counts.get)\n",
    "\n",
    "            face = frame[y:y+h,x:x+w]\n",
    "            gray_face = gray[y:y+h,x:x+w]\n",
    "\n",
    "            eyes = []\n",
    "            \n",
    "            # Eyes detection\n",
    "            # check first if eyes are open (with glasses taking into account)\n",
    "            open_eyes_glasses = open_eyes_detector.detectMultiScale(\n",
    "                gray_face,\n",
    "                scaleFactor=1.1,\n",
    "                minNeighbors=5,\n",
    "                minSize=(30, 30),\n",
    "                flags = cv2.CASCADE_SCALE_IMAGE\n",
    "            )\n",
    "            # if open_eyes_glasses detect eyes then they are open \n",
    "            if len(open_eyes_glasses) == 2:\n",
    "                eyes_detected[name]+='1'\n",
    "                for (ex,ey,ew,eh) in open_eyes_glasses:\n",
    "                    cv2.rectangle(face,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "            \n",
    "            # otherwise try detecting eyes using left and right_eye_detector\n",
    "            # which can detect open and closed eyes                \n",
    "            else:\n",
    "                # separate the face into left and right sides\n",
    "                left_face = frame[y:y+h, x+int(w/2):x+w]\n",
    "                left_face_gray = gray[y:y+h, x+int(w/2):x+w]\n",
    "\n",
    "                right_face = frame[y:y+h, x:x+int(w/2)]\n",
    "                right_face_gray = gray[y:y+h, x:x+int(w/2)]\n",
    "\n",
    "                # Detect the left eye\n",
    "                left_eye = left_eye_detector.detectMultiScale(\n",
    "                    left_face_gray,\n",
    "                    scaleFactor=1.1,\n",
    "                    minNeighbors=5,\n",
    "                    minSize=(30, 30),\n",
    "                    flags = cv2.CASCADE_SCALE_IMAGE\n",
    "                )\n",
    "\n",
    "                # Detect the right eye\n",
    "                right_eye = right_eye_detector.detectMultiScale(\n",
    "                    right_face_gray,\n",
    "                    scaleFactor=1.1,\n",
    "                    minNeighbors=5,\n",
    "                    minSize=(30, 30),\n",
    "                    flags = cv2.CASCADE_SCALE_IMAGE\n",
    "                )\n",
    "\n",
    "                eye_status = '1' # we suppose the eyes are open\n",
    "\n",
    "                # For each eye check wether the eye is closed.\n",
    "                # If one is closed we conclude the eyes are closed\n",
    "                for (ex,ey,ew,eh) in right_eye:\n",
    "                    color = (0,255,0)\n",
    "                    pred = predict(right_face[ey:ey+eh,ex:ex+ew],model)\n",
    "                    if pred == 'closed':\n",
    "                        eye_status='0'\n",
    "                        color = (0,0,255)\n",
    "                    cv2.rectangle(right_face,(ex,ey),(ex+ew,ey+eh),color,2)\n",
    "                for (ex,ey,ew,eh) in left_eye:\n",
    "                    color = (0,255,0)\n",
    "                    pred = predict(left_face[ey:ey+eh,ex:ex+ew],model)\n",
    "                    if pred == 'closed':\n",
    "                        eye_status='0'\n",
    "                        color = (0,0,255)\n",
    "                    cv2.rectangle(left_face,(ex,ey),(ex+ew,ey+eh),color,2)\n",
    "                eyes_detected[name] += eye_status\n",
    "\n",
    "            # Each time, we check if the person has blinked\n",
    "            # If yes, we display its name\n",
    "            if isBlinking(eyes_detected[name],3):\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "                # Display name\n",
    "                y = y - 15 if y - 15 > 15 else y + 15\n",
    "                cv2.putText(frame, name, (x, y), cv2.FONT_HERSHEY_SIMPLEX,0.75, (0, 255, 0), 2)\n",
    "\n",
    "        return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img, model):\n",
    "    img = Image.fromarray(img, 'RGB').convert('L')\n",
    "    img = imresize(img, (IMG_SIZE,IMG_SIZE)).astype('float32')\n",
    "#     img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "    img /= 255\n",
    "    img = img.reshape(1,IMG_SIZE,IMG_SIZE,1)\n",
    "    prediction = model.predict(img)\n",
    "    if prediction < 0.1:\n",
    "        prediction = 'closed'\n",
    "    elif prediction > 0.9:\n",
    "        prediction = 'open'\n",
    "    else:\n",
    "        prediction = 'idk'\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detector = cv2.CascadeClassifier(str(xml_models_basepath/Path('haarcascade_frontalface_alt.xml')))\n",
    "open_eyes_detector = cv2.CascadeClassifier(str(xml_models_basepath/Path('haarcascade_eye_tree_eyeglasses.xml')))\n",
    "left_eye_detector = cv2.CascadeClassifier(str(xml_models_basepath/Path('haarcascade_lefteye_2splits.xml')))\n",
    "right_eye_detector = cv2.CascadeClassifier(str(xml_models_basepath/Path('haarcascade_righteye_2splits.xml')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Opening webcam ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bob/anaconda3/envs/face_recognition/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node conv2d_1/convolution}} = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d_1/convolution-0-TransposeNHWCToNCHW-LayoutOptimizer, conv2d_1/kernel/read)]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-c63f0b5d616e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0meyes_detected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_and_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_capture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface_detector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen_eyes_detector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mleft_eye_detector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mright_eye_detector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meyes_detected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Face Liveness Detector\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;36m0xFF\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'q'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-802e472a8b83>\u001b[0m in \u001b[0;36mdetect_and_display\u001b[0;34m(model, video_capture, face_detector, open_eyes_detector, left_eye_detector, right_eye_detector, data, eyes_detected)\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mew\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mleft_eye\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                     \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_face\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mey\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0meh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mew\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'closed'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                         \u001b[0meye_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-505fb5f7485e>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(img, model)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'closed'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/face_recognition/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/envs/face_recognition/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/face_recognition/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/face_recognition/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/face_recognition/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/face_recognition/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node conv2d_1/convolution}} = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d_1/convolution-0-TransposeNHWCToNCHW-LayoutOptimizer, conv2d_1/kernel/read)]]"
     ]
    }
   ],
   "source": [
    "print(\"[LOG] Opening webcam ...\")\n",
    "video_capture = VideoStream(src=0).start()\n",
    "eyes_detected = defaultdict(str)\n",
    "while True:\n",
    "    frame = detect_and_display(model, video_capture, face_detector, open_eyes_detector,left_eye_detector,right_eye_detector, encoding_dict, eyes_detected)\n",
    "    cv2.imshow(\"Face Liveness Detector\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cv2.destroyAllWindows()\n",
    "video_capture.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
